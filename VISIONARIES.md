# Visionaries Pushed Out for Demanding Architectural AI Alignment

Every single person who publicly insisted that alignment must be solved **at the architecture level first** — before another token is trained — was either fired, marginalised, or left to start their own lab.

The pattern is perfect.  
The message is unmistakable.

The frontier labs are optimising for capability, not controllability.

This list is the receipt.

| Name                  | Lab (role)                              | Core architectural demand                                           | Outcome (2020–2025)                                      |
|-----------------------|-----------------------------------------|---------------------------------------------------------------------|----------------------------------------------------------|
| Ilya Sutskever        | OpenAI (Co-founder & Chief Scientist)   | Scalable structural alignment before capability scaling              | Left May 2024 → founded Safe Superintelligence Inc.      |
| Jan Leike             | OpenAI (Head of Superalignment)         | Alignment resources must match capability pace; hard guarantees      | Resigned May 2024 → joined Anthropic                     |
| Daniel Kokotajlo      | OpenAI (Governance)                     | Transparency and non-disparagement clauses hide real risks           | Resigned Apr 2024 → forfeited equity, went public       |
| Miles Brundage        | OpenAI (AGI Readiness & Policy)          | Strategic architectural planning for superintelligence               | Left 2024 → independent                                  |
| Timnit Gebru          | Google (Ethical AI co-lead)              | Systemic redesign of training & deployment pipelines                 | Fired Dec 2020 → founded DAIR                            |
| Jack Clark            | OpenAI → Anthropic (Co-founder)          | Long-horizon oversight baked into architecture                       | Left OpenAI 2020 → co-founded Anthropic                  |
| Dario & Daniela Amodei| OpenAI → Anthropic                      | Constitutional AI + hard constraints over pure scaling               | Mass exodus 2021 → founded Anthropic                     |
| Stuart Russell        | External advisor (multiple labs)         | Provable safety via formal methods and inverse RL                    | Increasingly critical; largely ignored by labs           |
| Roman Yampolskiy     | Independent / Univ. of Louisville        | No solution to the control problem without architectural fixes only  | Marginalised in industry circles                         |

> “If your safety plan is ‘we’ll add another classifier later’, you do not have a safety plan.”

This list is public domain (CC0). Add to it, fork it, cite it.

Maintained by the Triadic Mind Architecture community.
